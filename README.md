# Netflix Analytics Pipeline

A data engineering project for processing Netflix analytics data using Postgres and Python.

## ğŸ“‹ Overview

This project provides:
- âœ… Docker-based Postgres database setup
- âœ… Database schema initialization (raw, staging, intermediate, analytics)
- âœ… Raw data tables for Netflix titles, ratings, and viewing history
- âœ… Python scripts for database operations and sample data generation
- âœ… Docker Compose setup for easy development

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA PIPELINE ARCHITECTURE                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Raw CSV/JSON â”‚
    â”‚     Files     â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ Load data
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Postgres     â”‚
    â”‚  RAW Schema   â”‚
    â”‚               â”‚
    â”‚  â€¢ netflix_   â”‚
    â”‚    titles     â”‚
    â”‚  â€¢ netflix_   â”‚
    â”‚    ratings    â”‚
    â”‚  â€¢ netflix_   â”‚
    â”‚    viewing_   â”‚
    â”‚    history    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ (Future: dbt transformations)
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Postgres     â”‚
    â”‚ STAGING Schemaâ”‚
    â”‚               â”‚
    â”‚  (Future)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
netflix-pipeline/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # Main project documentation
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ Dockerfile                   # Docker image definition
â”œâ”€â”€ ğŸ“„ docker-compose.yml           # Docker Compose configuration
â”œâ”€â”€ ğŸ“„ Makefile                     # Common commands
â”œâ”€â”€ ğŸ“„ .gitignore                   # Git ignore rules
â”œâ”€â”€ ğŸ“„ .dockerignore                # Docker ignore rules
â”‚
â”œâ”€â”€ ğŸ“ scripts/                     # Setup & Utility Scripts
â”‚   â”œâ”€â”€ ğŸ“œ setup_postgres.sql       # Postgres schema setup
â”‚   â”œâ”€â”€ ğŸ“œ init-db.sql              # Postgres initialization script (Docker)
â”‚   â”œâ”€â”€ ğŸ run_postgres_sql.py      # Python script to run SQL files
â”‚   â””â”€â”€ ğŸ sample_data_generator.py # Generate test data
â”‚
â””â”€â”€ ğŸ“ data/                        # Data Files (gitignored)
```

## ğŸš€ Getting Started

### Prerequisites

- Docker and Docker Compose installed

### Quick Start with Docker (Recommended)

1. **Clone the repository:**
```bash
git clone <your-repo-url>
cd nexflix-pipeline
```

2. **Start the services:**
```bash
make docker-up
# or
docker-compose up -d
```

3. **Access the application container:**
```bash
make docker-shell
# or
docker-compose exec netflix-pipeline-app bash
```

4. **Verify Postgres is running:**
```bash
# From inside the container or host
docker-compose exec -T postgres psql -U postgres -d netflix_analytics -c "\dn"
```

You should see the schemas: `raw`, `staging`, `intermediate`, `analytics`

5. **Check raw tables:**
```bash
docker-compose exec -T postgres psql -U postgres -d netflix_analytics -c "\dt raw.*"
```

### Manual Installation (Without Docker)

**Prerequisites:**
- Postgres database (local or remote)
- Python 3.11+
- psql CLI (or use the Python alternative script)

1. **Install Python dependencies:**
```bash
make install
# or
pip install -r requirements.txt
```

2. **Set up Postgres database:**

   **First, create the database:**
   ```bash
   # Connect to Postgres (using default postgres database)
   psql -U postgres
   
   # In psql, create the database:
   CREATE DATABASE netflix_analytics;
   \q
   ```

   **Then set up schemas using Python script:**
   
   Set environment variables with your Postgres credentials:
   ```bash
   export POSTGRES_HOST="localhost"
   export POSTGRES_PORT="5432"
   export POSTGRES_DATABASE="netflix_analytics"
   export POSTGRES_USER="your_username"
   export POSTGRES_PASSWORD="your_password"
   
   # Run the setup script
   python scripts/run_postgres_sql.py scripts/setup_postgres.sql
   ```

   This script will create:
   - Schemas: `raw`, `staging`, `intermediate`, `analytics`
   - Raw tables: `netflix_titles`, `netflix_ratings`, `netflix_viewing_history`

## ğŸ“Š Database Schema

### Raw Schema

The `raw` schema contains the initial data tables:

- **netflix_titles** - Netflix titles catalog
- **netflix_ratings** - User ratings data
- **netflix_viewing_history** - Viewing history data

All tables include a `_load_timestamp` column that tracks when records were loaded.

## ğŸ³ Docker Commands

> **Note:** The default Postgres port is set to 5433 to avoid conflicts with local Postgres instances.

### Start services
```bash
make docker-up
# or
docker-compose up -d
```

### Stop services
```bash
make docker-down
# or
docker-compose down
```

### Access the application container
```bash
make docker-shell
# or
docker-compose exec netflix-pipeline-app bash
```

### Access Postgres directly
```bash
# From inside container
docker-compose exec postgres psql -U postgres -d netflix_analytics

# From host machine (if you have psql installed)
psql -h localhost -p 5433 -U postgres -d netflix_analytics
```

### View logs
```bash
make docker-logs
# or
docker-compose logs -f app
docker-compose logs -f postgres
```

### Rebuild containers
```bash
make docker-build
# or
docker-compose build --no-cache
docker-compose up -d
```

### Clean up volumes (âš ï¸ deletes all data)
```bash
docker-compose down -v
```

## ğŸ“ Makefile Commands

```bash
make help              # Show all available commands
make install           # Install Python dependencies
make setup             # Run initial Postgres setup
make generate-data      # Generate sample data
make docker-up         # Start Docker containers
make docker-down       # Stop Docker containers
make docker-shell      # Access application container shell
make docker-logs       # View container logs
make docker-build      # Rebuild Docker containers
```

## ğŸ”„ Workflow

1. **Start Services**: Use `make docker-up` to start Postgres and the app container
2. **Load Data**: Load raw data into the `raw` schema tables
3. **Generate Sample Data**: Use `make generate-data` to create test data
4. **Query Data**: Access Postgres to query and analyze the data

## âš ï¸ Known Issues

### Port 5432 Already in Use

**Issue:** Error: `bind: address already in use` when starting Postgres container.

**Solution:** The default port is already set to 5433 to avoid conflicts. If you still get this error:
1. Check if port 5433 is available: `lsof -i :5433`
2. Change the port in `.env`: `POSTGRES_PORT=5434`
3. Or stop your local Postgres: `sudo service postgresql stop`

## ğŸ“„ License

MIT License

## ğŸ‘¤ Author

Your Name
